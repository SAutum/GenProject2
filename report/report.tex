\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{exercises}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\begin{document}
 
% --------------------------------------------------------------
%
%                         Start here
%
% --------------------------------------------------------------
 
\title{Generative Models: Project 2} % replace with the problem you are writing up
\author{Hongli Lin (kag52rer), Michele Paterson (mipat104),\\ Sandro Jose Leal Miudo (salea100), Sven Klein (svkle100)} % replace with your name
\maketitle
\section{Choice of Methods}
In this project we used a Generative Adversial Network (GAN) to generate fibre orientation maps (FOM). For the specific choice of model we used the basic variant from the lecture with gradient ascent for better gradients. This choice was made since it was the easiest to implement. However other models such as Stable Diffusion or a Wasserstein GAN would probably achieve better results. For image generation we used to different methods. The first method just generates images from the target distribuiton unconditionally. The second method uses the corresponding transmittance maps as conditional inputs into our model. For this we decided to use the Pix2Pix method. This choice was made again because it is easy to implement, but also because we already had paired transmittance maps and FOMs, so Pix2Pix should perform well compared to other methods. 
\section{Problems}
One major problem we had was that training would collapse. It turned out that this was due to our descriminator being to good. That would lead to it correctly classifying every image and the generator could never improve. We solved this by making several changes to the descriminator. We reduced the amount of layers and also added droupout layers to it. These changes lead to a more balanced training where the losses of the generator and the descriminator were more balanced.
Another issue is that with some probability a tile from the FOM gets sampled where there is no brain tissue on it. This leads to the tile being completly black. Therefore generating a completly black tile will always fool the discriminator. This could lead to a mode collapse. We acknowledged the problem and highly recommend implementing counter measures like for example minibatch discrimination. These were not implemented simply because of time constraints.
\section{Results}
\subsection{Unconditional Generation}
Just by a visual inspection we can tell that the model learned at least something about the target distribution. It generates the already mentioned black images as well as images that look similiar to sampled patches from the FOMs. Since we dont know very much about the brain and it's cell structures we can't really comment on if the generateted images contain realistic structures or not. We also calculated the FID using the inception network and the validation set and achieved a FID of 4.18. This is not a very high score, but it also shows that we atleast somewhat captured the data distribution. Also since the inception network was not trained on FOMs the its features probably are not very meaningful. Ideally we would use a model which is familiar with FOMs as input. We thought of using the discriminator for that, but since the generator is specifically trained to fool the discriminator this also would not lead to a very accurate score.
\subsection{Conditional Generation}
For this task we have access to target images so we can perform an analysis by direct comparison. Just from a visual comparison we can already tell that the generated images are not close to the real images at all. They lack a lot of details and color. This is confirmed by calculating the mean squared error (MSE). We calculate an MSE of 1340 with a standard deviation of 1903. This indicates the predictions are mostly wrong. However we look at the structual similarity index we get an result of 0.45 with a standard deviation of 0.35. This score measures structural similarity on a scale of $[-1, 1]$. So it detected at least some similiarties between the images. This indicates that the model can reason about the structure of the FOM from the transimittance map, but struggles to predict the correct orientation of the nerve fibres. 
%\bibliography{references}
\end{document}
